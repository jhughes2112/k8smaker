#!/bin/bash

# If you have an SSD device, declare it here and the script will make sure it's mounted
# whereever DATAFOLDER is.  It's your job to make sure it's partitioned (use parted) and 
# formatted (use mkfs.ext4) first.  I don't do this for you, in case there's other stuff
# on the drive already.
#SSD="/dev/nvme0n1"
SSD="/dev/no-ssd-device"

# Even if you have no SSD, this is the root of where Docker and etcd data will live.
# You must specify this folder to be SOMEWHERE, as /docker and /etcd are mounted beneath it.
DATAFOLDER="/mnt/data"

# This script doesn't use a randomized bootstrap token, nor does it expire.  Slight security issue.
# Definitely change this, but it must be EXACTLY this many letters long.
BOOTSTRAPTOKEN="secret.bootstraptokenxx"

# To select your cluster, you do this: kubectl config use-context CLUSTERNAME
CLUSTERNAME="testcluster"

# Can be any subnet, but CLUSTERDNS needs to be on the SERVICESUBNET.  This is where pods will be
# running, so don't pick a subnet you already have devices on unless you know what you're doing.
SERVICESUBNET="10.99.0.0/16"
CLUSTERDNS="10.99.0.10"

# Can be any subnet, probably non-routable only.  Google up "non-routable ip addresses"
PODSUBNET="10.98.0.0/16"

# This is the username used when SSH'ing into machines
SSHUSERNAME="ubuntu"

# Used only with baremetal, this is the IP address that you want to expose as the load balanced IP
# that you connect to with kubectl (and other nodes will use to talk to the control nodes).  If you
# want this to be an external IP address, fine.  If you want to always ssh into a cluster node first,
# pick an IP that is in the SERVICESUBNET.
#CONTROLPLANEVIP="10.99.0.11"
CONTROLPLANEVIP="10.0.0.251"

# You cannot change this value after creating the cluster.  So it should always be a name not an IP.
# BareMetal: Leave it alone.  Ideally, add control-nodes as a DNS entry so it resolves properly, but
# if not, we patch /etc/hosts to point control-nodes to the CONTROLPLANEVIP manually on each node.
# This is the software loade balanced vIP address.  Works fine whether you have 1 or more control nodes.
# AWS: Put the actual name of your internal control plane load balancer here, if you plan
# to use one.  Otherwise leave it alone.  You can always come back and sub in the LB's IP address
# but those sometimes change without notice, so YMMV.
CONTROLPLANEENDPOINT="control-nodes"
#CONTROLPLANEENDPOINT="internal-foo-1709887039.us-east-1.elb.amazonaws.com"

# These let you control the versions of packages being installed.  I have no way to guarantee any
# combination will work except those I have checked in together (which means I tested them myself).
KUBERNETESVERSION="1.20.4"
CALICOVERSION="v3.17.3"
CONTOURVERSION="1.12"
KUBEVIPVERSION="0.3.2"
