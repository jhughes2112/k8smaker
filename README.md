
# k8smaker
Automating all the things!

This is a personal project to make it easier to stand up a Kubernetes cluster from a fresh install of Ubuntu.  The number of times I have done these dozens of steps, I cannot count.  Rather than keep the cheat sheet of commands with me, I wrote this set of scripts so I can essentially wipe out a machine, pull down this repo, and launch the **k8smaker_init** script and BAM, Kubernetes cluster on one machine.  Run **k8smaker_addworker yournode** and now you have a two node cluster.  Repeat as desired.  Easy!

Furthermore, the documentation for all the things you should do to setup a machine for Kubernetes is fairly scattered and hard-won.  Every step is important, of course, but if you have read some of the walkthroughs on installing Kubernetes, most of them include relatively few of the preconditioning steps.  This gathers everything into one place and does all the things that I've seen recommended at each step, in the right order.  All these scripts are intended to run from the first machine in the k8s cluster, which we'll call `control-node`.  

# Quickstart
## Init a cluster
- `scp ~/.ssh/PRIVATE_KEY_NAME ubuntu@CONTROL_NODE/.ssh/id_rsa` You must be able to ssh from the control node to each worker node, so copy up your private key to the control node.
- `ssh CONTROL_NODE`
- `chmod 0600 ~/.ssh/id_rsa` sets the permissions for your private key to access other worker nodes.  At this point, you could test by ssh'ing to one of your workers from the control node.  Just remember to return to the control node before doing the next step.
- `git clone https://github.com/jhughes2112/k8smaker.git`
- `cd k8smaker`
- Edit **k8smaker_config** and set cluster name, username you will use for ssh (often **ubuntu**), change the bootstrap token, change the control node name to a load balancer DNS entry if you plan to use one, etc.
- `./k8smaker_init_aws` will fully configure and set up a Kubernetes cluster on the local machine with etcd,control,worker roles.

## For each worker node
- `ssh CONTROL_NODE`
- `cd k8smaker`
- ./k8smaker_addworker ip-172-16-1-144.ec2.internal` will construct a script to run on the specified host, copy it over then run it for you.  All scripts are stored both on the control-node in your home folder as well as on the remote host it was executed on.

## Cluster Configuration
 - Kubernetes 1.18.4
 - Ubuntu 18.04LTS
 - Bare metal or anything else
 - etcd and control plane are stacked on control nodes
 -- A smaller machine will suffice to manage the cluster (2+CPU, 2+GB RAM)
 -- Setup for HA control cluster, even if you only have one to start with
 - pods run on worker nodes
 -- Use beefy machines for these
 - **Calico** for networking automatically configured to talk to secured etcd
 - **Istio 1.6.3** for ingress and mesh routing

## Prerequisites for k8smaker
 - All nodes need to be a `systemd`-based Ubuntu 18.04LTS (or similar Debian distro)
 - All nodes need [OpenSSH](https://linuxize.com/post/how-to-enable-ssh-on-ubuntu-18-04/) installed, but don't worry about setting up keys.  That's built in.
 - All nodes need the same username (often `ubuntu` but can be anything), and needs `sudo` access.
 - All scripts are to be executed on the `control-node` machine.

## k8smaker_config
Edit this file to set your cluster name, username, etc.  This is shared across all the scripts that need it, not all do.  See comments for what each variable means.

In my case, I run on bare metal that boots from a slow drive, but have fast data drives.  So this script allows me to configure the device to mount at the data folder location.  If this doesn't apply to you, point it at anything other than a block device and it'll skip that part.  You still want to provide it a root folder for all the data, since /var/lib/docker and /var/lib/etcd get mapped there.

## k8smaker_precondition
This script is executed on the `control-node` when it is initializing the k8s cluster the first time.  It is also executed on each worker as it is added.  Consequently, it's a great place to throw additional tweaks to the configuration if you want.

## k8smaker_init_aws/baremetal
You run this **on the machine** you want to be the `control-node`.  This script preps the machine, which configures apt, installs a few packages it needs to do the install like curl, docker, kubeadm, kubectl, kubelet, etc.  Several .yaml files are generated by this process and scattered across the file system.  This script does not go out of its way to delete files on disk that might exist, but it will install/upgrade things.  If you already have docker images running on this machine, the /var/lib/docker folder will move, so you are going to want to stop all your containers first, then move the contents of /var/lib/docker to $DATAFOLDER/docker.

One little quirk is that Istio refuses to complete installation without a worker node to schedule some pods onto.  Consequently, the `control-node` always starts off as a full etcd,control,worker role, which means you have a fully working cluster immediately.  There's a setting in the config file to convert it to etcd,control when the first worker node is added.

## k8smaker_addworker_aws/baremetal
You run this on the `control-node`, passing it the **IP or hostname** of the node you want to be a worker for the cluster.  First, it generates a Bash script called at `~/CLUSTERNAME/k8smaker_addworker_HOSTNAME`, so you can inspect what it's doing.  The script then configures the new node with an ssh key the cluster uses for passwordless operation, copies the customized Bash script over, then remote executes it.  You will probably need to accept the SSH signature, mayneed to provide an SSH password, then give the password to allow sudo access on that machine, since some commands require root to configure it.

## k8smaster_drain
Run this on the `control-node`, pass in the **nodename** to drain.  This simply tries to drain pods off the specified node.  It's still part of the cluster, just not in use.  Some things don't drain nicely, but I'll handle that better in the future--hence the script.  If you have a node you want to take down, drain it first.  This will eventually manage re-scaling stateful apps that need to be rescheduled to other nodes, but for now just executes the correct kubectl drain command for you.

## k8smaster_undrain
Run this on the `control-node`, pass in the **nodename** to drain.  This puts a drained node back into service.  Simple as that.  Technically, the right name is uncordon, but it ruins the symmetry of the naming, sorry.

## k8smaster_deleteall
You run this on the `control-node`, passing it the **IP or hostname** of the node you want to completely remove from the cluster.  This constructs a script to run on the specified host, copies and remotely executes it on that host, then attempts to delete the node from the cluster on the off-chance it's unable to reach it (crashed, network down, whatever).  If it successfully executes the script, everything related to Docker, Kubernetes, etcd, and so forth is deleted off the machine and uninstalled.  It's vicious, so be careful.  Always try to drain first, or you may experience data loss depending on your workload.  Note, this will also work on control nodes (you can pass `localhost` to it to wipe the current machine).

## Disclaimers
Tested on Ubuntu 18.04LTS only.  May run on any reasonably vanilla `systemd` based Debian-based Linux.  Do not run these scripts on anything except a newly installed machine.  **Data loss is very possible.**  Some parts of these scripts run as root.  As with anything you download off the internet, running a script as `root` is dangerous and until it's proven trustworthy, it may destroy anything or everything it touches.  I promise this script doesn't intend to do that, but why should  you trust me?  **Read it over first, and if you see anything you are concerned with, don't run it!**  Even better, fix it and send me a pull request.

## TODO
- Add a script to add an etcd,control node to the cluster, or change init to start with a 1-, 3- or 5-node control plane.
