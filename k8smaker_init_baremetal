#!/bin/bash

if [[ $(whoami) = "root" ]]; then
   echo "Must not run as root."
   exit 1
fi

source ./k8smaker_config

mkdir -p ~/"$CLUSTERNAME"
INITSCRIPT=~/"$CLUSTERNAME/kubeadm.yaml"

# this picks up your external facing IP address and hostname
EXTINTERFACE=$(ip route get 8.8.8.8 | awk '{print $5}')
CONTROLNODEIP=$(ip route get 8.8.8.8 | awk '{print $7}')

# Delete an existing k8s install while leaving Docker alone.  Doesn't get rid of all the extra things we did, but those aren't harmful to reinstalling.
echo "You have 5 seconds before this script deletes all Kubernetes and Etcd data.  Control-C To Abort!"
sleep 5

# Generate an ssh key just for connecting to other nodes and managing their startup
mkdir -p $HOME/.ssh
chmod 0700 $HOME/.ssh
ssh-keygen -t rsa -f "$HOME/.ssh/$CLUSTERNAME" -N '' <<< y

source ./k8smaker_precondition

# Give user access to docker
sudo usermod -aG docker $USER

# Note, the bootstrap token is insecure because it's not randomized nor does it expire (ttl).
cat >"$INITSCRIPT" <<EOF
kind: InitConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ${BOOTSTRAPTOKEN}
  ttl: 0s
  usages:
  - signing
  - authentication
localAPIEndpoint:
  advertiseAddress: ${CONTROLNODEIP}
  bindPort: 6443
---
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
apiServer:
  timeoutForControlPlane: 4m0s
certificatesDir: /etc/kubernetes/pki
clusterName: $CLUSTERNAME
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kubernetesVersion: v$KUBERNETESVERSION
networking:
  dnsDomain: cluster.local
  serviceSubnet: $SERVICESUBNET
  podSubnet: $PODSUBNET
controlPlaneEndpoint: $CONTROLPLANEENDPOINT
scheduler: {}
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- $CLUSTERDNS
clusterDomain: cluster.local
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
logging: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
EOF

# Generate the manifest for kube-vip so it can handle load balancing the control node interfaces.
# This manifest is also added to any future control nodes, which is why it is broken out for easy access.
sudo docker run --network host --rm plndr/kube-vip:$KUBEVIPVERSION manifest pod --interface $EXTINTERFACE --vip $CONTROLPLANEVIP --controlplane --services --arp --leaderElection | tee ~/"$CLUSTERNAME"/kube-vip.yaml
sudo mkdir -p /etc/kubernetes/manifests
sudo cp ~/"$CLUSTERNAME"/kube-vip.yaml /etc/kubernetes/manifests/kube-vip.yaml
sudo chmod 600 /etc/kubernetes/manifests/kube-vip.yaml

# Actually create the cluster with just the control,etcd on this node, but do so AS root in root's own shell.
# Otherwise, you get permissions issues in your ~/.kube folder.
sudo -i kubeadm init --config="$INITSCRIPT" --upload-certs

# Configure kubectl to talk to the local cluster in your user login.
mkdir -p $HOME/.kube
sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

pushd ~/"$CLUSTERNAME"

# Install Calico operator for pod networking
cat >calico2.yaml <<EOF
kind: Installation
apiVersion: operator.tigera.io/v1
metadata:
  name: default
spec:
  # Configures Calico networking.
  calicoNetwork:
    # Note: The ipPools section cannot be modified post-install.
    ipPools:
    - blockSize: 26
      cidr: $PODSUBNET
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
EOF

curl https://docs.projectcalico.org/manifests/tigera-operator.yaml -o calico1.yaml
kubectl apply -f calico1.yaml
kubectl apply -f calico2.yaml

# Download and install Contour for Ingress
echo "Installing Contour"
curl -L https://raw.githubusercontent.com/projectcontour/contour/release-$CONTOURVERSION/examples/render/contour.yaml -o contour.yaml
kubectl apply -f contour.yaml

popd

# allow this node to run workloads, so we can do single node clusters for testing.
source ./k8smaker_setcontrolplane $(hostname) schedule

echo && echo "You can now use kubectl to work on the cluster, however your group changed so docker commands won't connect until you logout and log back in. To control this k8s cluster remotely, install kubectl and copy down the ~/.kube/config file to your local machine."
