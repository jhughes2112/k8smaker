#!/bin/bash

if [[ $(whoami) = "root" ]]; then
   echo "Must not run as root."
   exit 1
fi

source ./k8smaker_config

mkdir -p ~/"$CLUSTERNAME"
INITSCRIPT=~/"$CLUSTERNAME/kubeadm.yaml"
CALICOSCRIPT1=~/"$CLUSTERNAME/calico1.yaml"
CALICOSCRIPT2=~/"$CLUSTERNAME/calico2.yaml"

# this picks up your external facing IP address and hostname
CONTROLNODEIP=$(ip route get 8.8.8.8 | awk '{print $7}')
THISHOSTNAME=$(hostname -f)
echo "Using $THISHOSTNAME at IP $CONTROLNODEIP"

# Delete an existing k8s install while leaving Docker alone.  Doesn't get rid of all the extra things we did, but those aren't harmful to reinstalling.
echo "You have 5 seconds before this script deletes all Kubernetes and Etcd data.  Control-C To Abort!"
sleep 5

# Generate an ssh key just for connecting to other nodes and managing their startup
mkdir -p $HOME/.ssh
chmod 0700 $HOME/.ssh
ssh-keygen -t rsa -f "$HOME/.ssh/$CLUSTERNAME" -N '' <<< y

source ./k8smaker_precondition

# Give user access to docker
sudo usermod -aG docker $USER

# Note, the bootstrap token is insecure because it's not randomized nor does it expire (ttl).
cat >"$INITSCRIPT" <<EOF
kind: InitConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: $BOOTSTRAPTOKEN
  ttl: 0s
  usages:
  - signing
  - authentication
localAPIEndpoint:
  advertiseAddress: $CONTROLNODEIP
  bindPort: 6443
---
kind: JoinConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
caCertPath: /etc/kubernetes/pki/ca.crt
discovery:
  bootstrapToken:
    apiServerEndpoint: $CONTROLPLANEENDPOINT:6443
    token: $BOOTSTRAPTOKEN
    unsafeSkipCAVerification: true
  timeout: 5m0s
  tlsBootstrapToken: $BOOTSTRAPTOKEN
---
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
apiServer:
  timeoutForControlPlane: 4m0s
certificatesDir: /etc/kubernetes/pki
clusterName: $CLUSTERNAME
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kubernetesVersion: v$KUBERNETESVERSION
networking:
  dnsDomain: cluster.local
  serviceSubnet: $SERVICESUBNET
  podSubnet: $PODSUBNET
controlPlaneEndpoint: $CONTROLPLANEENDPOINT
scheduler: {}
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- $CLUSTERDNS
clusterDomain: cluster.local
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
logging: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
EOF

# Actually create the cluster with just the control,etcd on this node, but do so AS root in root's own shell.
# Otherwise, you get permissions issues in your ~/.kube folder.
sudo -i kubeadm init --config="$INITSCRIPT" --upload-certs

# Configure kubectl to talk to the local cluster in your user login.
mkdir -p $HOME/.kube
sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Install Calico w/etcd for networking
cat >"$CALICOSCRIPT2" <<EOF
kind: Installation
apiVersion: operator.tigera.io/v1
metadata:
  name: default
spec:
  # Configures Calico networking.
  calicoNetwork:
    # Note: The ipPools section cannot be modified post-install.
    ipPools:
    - blockSize: 26
      cidr: $PODSUBNET
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
EOF

curl https://docs.projectcalico.org/manifests/tigera-operator.yaml -o "$CALICOSCRIPT1"
kubectl apply -f "$CALICOSCRIPT1"
kubectl apply -f "$CALICOSCRIPT2"

# Download and install Contour for Ingress
echo "Installing Contour"
pushd ~/"$CLUSTERNAME"
curl -L https://raw.githubusercontent.com/projectcontour/contour/release-$CONTOURVERSION/examples/render/contour.yaml -o contour.yaml
kubectl apply -f contour.yaml
popd

# allow this node to run workloads, otherwise Istio will not start up.
source ./k8smaker_setcontrolnode $(hostname) schedule

echo && echo "You can now use kubectl to work on the cluster, however your group changed so docker commands won't connect until you logout and log back in. To control this k8s cluster remotely, install kubectl and copy down the ~/.kube/config file to your local machine.  Be sure to change the 'server' line to point to $CONTROLNODEIP"
