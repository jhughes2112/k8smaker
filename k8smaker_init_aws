#!/bin/bash

if [[ $(whoami) = "root" ]]; then
   echo "Must not run as root."
   exit 1
fi

source ./k8smaker_config

# AWS nodes must have $(hostname) == $(hostname -f) because somewhere in the middle of initialization, someone is using the short name, and it blows up.
sudo hostnamectl set-hostname $(curl -s http://169.254.169.254/latest/meta-data/local-hostname)

mkdir -p ~/"$CLUSTERNAME"
INITSCRIPT=~/"$CLUSTERNAME/kubeadm.yaml"
CONTROLNODEIP=$(ip route get 8.8.8.8 | awk '{print $7}')

# Delete an existing k8s install while leaving Docker alone.  Doesn't get rid of all the extra things we did, but those aren't harmful to reinstalling.
echo "You have 5 seconds before this script deletes all Kubernetes and Etcd data.  Control-C To Abort!"
sleep 5

# Generate an ssh key just for connecting to other nodes and managing their startup
mkdir -p $HOME/.ssh
chmod 0700 $HOME/.ssh
ssh-keygen -t rsa -f "$HOME/.ssh/$CLUSTERNAME" -N '' <<< y

source ./k8smaker_precondition

# Give user access to docker
sudo usermod -aG docker $USER

# Note, the bootstrap token is insecure because it's not randomized nor does it expire (ttl).
cat >"$INITSCRIPT" <<EOF
kind: InitConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ${BOOTSTRAPTOKEN}
  ttl: 0s
  usages:
  - signing
  - authentication
localAPIEndpoint:
  advertiseAddress: ${CONTROLNODEIP}
  bindPort: 6443
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: external
---
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
apiServer:
  timeoutForControlPlane: 4m0s
  extraArgs:
    cloud-provider: external
certificatesDir: /etc/kubernetes/pki
clusterName: $CLUSTERNAME
controllerManager:
  extraArgs:
    cloud-provider: external
    configure-cloud-routes: "false"
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kubernetesVersion: v$KUBERNETESVERSION
networking:
  dnsDomain: cluster.local
  serviceSubnet: $SERVICESUBNET
  podSubnet: $PODSUBNET
controlPlaneEndpoint: $CONTROLPLANEENDPOINT
scheduler: {}
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- $CLUSTERDNS
clusterDomain: cluster.local
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
logging: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
EOF

# Actually create the cluster with just the control,etcd on this node, but do so AS root in root's own shell.
# Otherwise, you get permissions issues in your ~/.kube folder.
sudo -i kubeadm init --config="$INITSCRIPT" --upload-certs

# Configure kubectl to talk to the local cluster in your user login.
mkdir -p $HOME/.kube
sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

pushd ~/"$CLUSTERNAME"

# Install Calico operator for pod networking
cat >calico2.yaml <<EOF
kind: Installation
apiVersion: operator.tigera.io/v1
metadata:
  name: default
spec:
  # Configures Calico networking.
  calicoNetwork:
    # Note: The ipPools section cannot be modified post-install.
    ipPools:
    - blockSize: 26
      cidr: $PODSUBNET
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
EOF

curl https://docs.projectcalico.org/manifests/tigera-operator.yaml -o calico1.yaml
kubectl apply -f calico1.yaml
kubectl apply -f calico2.yaml

# Install AWS cloud provider and storage class "gp2" for EBS volumes
curl https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/master/manifests/rbac.yaml -o aws1.yaml
curl https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/master/manifests/aws-cloud-controller-manager-daemonset.yaml -o aws2.yaml
curl https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/storage-class/aws/default.yaml -o aws3.yaml
kubectl apply -f aws1.yaml
kubectl apply -f aws2.yaml
kubectl apply -f aws3.yaml

# Download and install Contour for Ingress
echo "Installing Contour"
curl -L https://raw.githubusercontent.com/projectcontour/contour/release-$CONTOURVERSION/examples/render/contour.yaml -o contour.yaml
# inject AWS annotation for NLB here for workers.
sed -i 's/service.beta.kubernetes.io\/aws-load-balancer-backend-protocol: tcp/service.beta.kubernetes.io\/aws-load-balancer-type: nlb/' contour.yaml
kubectl apply -f contour.yaml

popd

# allow this node to run workloads, so we can do single node clusters for testing.
source ./k8smaker_setcontrolplane $(hostname) schedule

echo && echo "You can now use kubectl to work on the cluster, however your group changed so docker commands won't connect until you logout and log back in. To control this k8s cluster remotely, install kubectl and copy down the ~/.kube/config file to your local machine."
