#!/bin/bash

if [[ $(whoami) = "root" ]]; then
   echo "Must not run as root."
   exit 1
fi

source ./k8smaker_config

# AWS nodes must have $(hostname) == $(hostname -f) because somewhere in the middle of initialization, someone is using the short name, and it blows up.
sudo hostnamectl set-hostname $(curl -s http://169.254.169.254/latest/meta-data/local-hostname)

mkdir -p ~/"$CLUSTERNAME"
INITSCRIPT=~/"$CLUSTERNAME/kubeadm.yaml"
CALICOSCRIPT=~/"$CLUSTERNAME/calicoetcd.yaml"

# this picks up your external facing IP address and hostname
CONTROLNODEIP=$(ip route get 8.8.8.8 | awk '{print $7}')
THISHOSTNAME=$(hostname -f)
echo "Using $THISHOSTNAME at IP $CONTROLNODEIP"

# Delete an existing k8s install while leaving Docker alone.  Doesn't get rid of all the extra things we did, but those aren't harmful to reinstalling.
echo "You have 5 seconds before this script deletes all Kubernetes and Etcd data.  Control-C To Abort!"
sleep 5

# Generate an ssh key just for connecting to other nodes and managing their startup
mkdir -p $HOME/.ssh
chmod 0700 $HOME/.ssh
ssh-keygen -t rsa -f "$HOME/.ssh/$CLUSTERNAME" -N '' <<< y

source ./k8smaker_precondition

# Give user access to docker
sudo usermod -aG docker $USER

# Note, the bootstrap token is insecure because it's not randomized nor does it expire (ttl).
cat >"$INITSCRIPT" <<EOF
kind: InitConfiguration
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: $BOOTSTRAPTOKEN
  ttl: 0s
  usages:
  - signing
  - authentication
localAPIEndpoint:
  advertiseAddress: $CONTROLNODEIP
  bindPort: 6443
nodeRegistration:
  kubeletExtraArgs:
    cloud-provider: aws
---
kind: ClusterConfiguration
apiServer:
  timeoutForControlPlane: 4m0s
  extraArgs:
    cloud-provider: aws
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: $CLUSTERNAME
controllerManager:
  extraArgs:
    cloud-provider: aws
    configure-cloud-routes: "false"
    address: 0.0.0.0
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kubernetesVersion: v$KUBERNETESVERSION
networking:
  dnsDomain: cluster.local
  serviceSubnet: $SERVICESUBNET
  podSubnet: $PODSUBNET
controlPlaneEndpoint: $CONTROLPLANEENDPOINT
scheduler:
  extraArgs:
    address: 0.0.0.0
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- $CLUSTERDNS
clusterDomain: cluster.local
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMinimumGCAge: 0s
logging: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
EOF

# Actually create the cluster with just the control,etcd on this node, but do so AS root in root's own shell.
# Otherwise, you get permissions issues in your ~/.kube folder.
sudo -i kubeadm init --config="$INITSCRIPT" --upload-certs

# Configure kubectl to talk to the local cluster in your user login.
mkdir -p $HOME/.kube
sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Install Calico w/etcd for networking
curl https://docs.projectcalico.org/manifests/calico-etcd.yaml -o "$CALICOSCRIPT"
sed -i 's%etcd_ca: ""   # "/calico-secrets/etcd-ca"%etcd_ca: "/calico-secrets/etcd-ca"%g' "$CALICOSCRIPT"
sed -i 's%etcd_cert: "" # "/calico-secrets/etcd-cert"%etcd_cert: "/calico-secrets/etcd-cert"%g' "$CALICOSCRIPT"
sed -i 's%etcd_key: ""  # "/calico-secrets/etcd-key"%etcd_key: "/calico-secrets/etcd-key"%g' "$CALICOSCRIPT"

SEDCMD="sed -i 's%etcd_endpoints: \"http://<ETCD_IP>:<ETCD_PORT>\"%etcd_endpoints: \"https://${CONTROLNODEIP}:2379\"%g' $CALICOSCRIPT"
eval "$SEDCMD"

ETCDKEY=$(sudo cat /etc/kubernetes/pki/etcd/peer.key | base64 -w 0)
SEDCMD="sed -i 's%# etcd-key: null%etcd-key: \"$ETCDKEY\"%g' $CALICOSCRIPT"
eval "$SEDCMD"

ETCDCERT=$(sudo cat /etc/kubernetes/pki/etcd/peer.crt | base64 -w 0)
SEDCMD="sed -i 's%# etcd-cert: null%etcd-cert: \"$ETCDCERT\"%g' $CALICOSCRIPT"
eval "$SEDCMD"

ETCDCA=$(sudo cat /etc/kubernetes/pki/etcd/ca.crt | base64 -w 0)
SEDCMD="sed -i 's%# etcd-ca: null%etcd-ca: \"$ETCDCA\"%g' $CALICOSCRIPT"
eval "$SEDCMD"

# Fix Calico's IP autodetection to actually find a valid interface.  Why this isn't the default blows my mind.
sed -i -e '/^            # Auto-detect the BGP IP address.*/i\            # Set auto-detect method to be publicly routable, whatever that interface may be\n            - name: IP_AUTODETECTION_METHOD\n              value: "can-reach=8.8.8.8"' $CALICOSCRIPT 

echo "Installing Calico, configuring calcicoctl"
kubectl apply -f "$CALICOSCRIPT"
sudo curl -L  https://github.com/projectcalico/calicoctl/releases/download/$CALICOVERSION/calicoctl -o /usr/local/bin/calicoctl
sudo chmod a+x /usr/local/bin/calicoctl
sudo mkdir -p /etc/calico
sudo tee /etc/calico/calicoctl.cfg >/dev/null <<EOF
apiVersion: projectcalico.org/v3
kind: CalicoAPIConfig
metadata:
spec:
  datastoreType: "kubernetes"
  kubeconfig: "$HOME/.kube/config"
EOF

# Download and install Contour for Ingress
echo "Installing Contour"
pushd ~/"$CLUSTERNAME"
curl -L  https://raw.githubusercontent.com/projectcontour/contour-operator/$CONTOURVERSION/examples/operator/operator.yaml -o contour-operator.yaml
kubectl apply -f contour-operator.yaml
popd

# allow this node to run workloads, otherwise Istio will not start up.
source ./k8smaker_setcontrolnode $(hostname) schedule

echo && echo "You can now use kubectl to work on the cluster, however your group changed so docker commands won't connect until you logout and log back in. To control this k8s cluster remotely, install kubectl and copy down the ~/.kube/config file to your local machine.  Be sure to change the 'server' line to point to $CONTROLNODEIP or $(dig +short myip.opendns.com @resolver1.opendns.com), whichever is publicly."
